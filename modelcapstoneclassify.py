# -*- coding: utf-8 -*-
"""ModelCapstoneClassify.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S5zH0QB1PAoAFFndYUiu_deYPtbZg-Gm
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
from torch.nn import functional as F

def MIL(y_pred, batch_size, is_transformer=0):
    loss = torch.tensor(0.)
    loss_intra = torch.tensor(0.)
    sparsity = torch.tensor(0.)
    smooth = torch.tensor(0.)
    if is_transformer==0:
        y_pred = y_pred.view(batch_size, -1)
    else:
        y_pred = torch.sigmoid(y_pred)

    for i in range(batch_size):
        anomaly_index = torch.randperm(30)
        normal_index = torch.randperm(30)

        y_anomaly = y_pred[i, :32][anomaly_index]
        y_normal  = y_pred[i, 32:][normal_index]

        y_anomaly_max = torch.max(y_anomaly) # anomaly
        y_anomaly_min = torch.min(y_anomaly)

        y_normal_max = torch.max(y_normal) # normal
        y_normal_min = torch.min(y_normal)

        loss += F.relu(1.-y_anomaly_max+y_normal_max)

        sparsity += torch.sum(y_anomaly)*0.00008
        smooth += torch.sum((y_pred[i,:31] - y_pred[i,1:32])**2)*0.00008
    loss = (loss+sparsity+smooth)/batch_size

    return loss

import torch
import torch.nn as nn
from torch.nn import functional as F



class Learner(nn.Module):
    def __init__(self, input_dim=2048, drop_p=0.0):
        super(Learner, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(drop_p),
            nn.Linear(512, 32),
            nn.ReLU(),
            nn.Dropout(drop_p),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.drop_p = drop_p
        self.weight_init()
        self.vars = nn.ParameterList()

        for i, param in enumerate(self.classifier.parameters()):
            self.vars.append(param)

    def weight_init(self):
        for layer in self.classifier:
            if type(layer) == nn.Linear:
                nn.init.xavier_normal_(layer.weight)

    def forward(self, x, vars=None):
        if vars is None:
            vars = self.vars
        x = F.linear(x, vars[0], vars[1])
        x = F.relu(x)
        x = F.dropout(x, self.drop_p, training=self.training)
        x = F.linear(x, vars[2], vars[3])
        x = F.dropout(x, self.drop_p, training=self.training)
        x = F.linear(x, vars[4], vars[5])
        return torch.sigmoid(x)

    def parameters(self):
        """
        override this function since initial parameters will return with a generator.
        :return:
        """
        return self.vars

import torch
import torch.nn as nn
from torch.nn import functional as F



class Learner2(nn.Module):
    def __init__(self, input_dim=2048, drop_p=0.0):
        super(Learner2, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(drop_p),
            nn.Linear(512, 32),
            nn.ReLU(),
            nn.Dropout(drop_p),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.drop_p = drop_p
        self.weight_init()
        self.vars = nn.ParameterList()
        self.filter1 = nn.LayerNorm(input_dim)

        for i, param in enumerate(self.classifier.parameters()):
            self.vars.append(param)

    def weight_init(self):
        for layer in self.classifier:
            if type(layer) == nn.Linear:
                nn.init.xavier_normal_(layer.weight)

    def forward(self, x, vars=None):
        if vars is None:
            vars = self.vars

        x1 = F.linear(x, vars[0], vars[1])
        x1 = F.relu(x1)
        x1 = F.dropout(x1, self.drop_p, training=self.training)
        x1 = F.linear(x1, vars[2], vars[3])
        x1 = F.dropout(x1, self.drop_p, training=self.training)
        x1 = F.linear(x1, vars[4], vars[5])

        x = self.relu(self.filter1(x))
        x2 = F.linear(x, vars[0], vars[1])
        x2 = F.relu(x2)
        x2 = F.dropout(x2, self.drop_p, training=self.training)
        x2 = F.linear(x2, vars[2], vars[3])
        x2 = F.dropout(x2, self.drop_p, training=self.training)
        x2 = F.linear(x2, vars[4], vars[5])

        x = (x1 + x2)/2.

        return torch.sigmoid(x)

    def parameters(self):
        """
        override this function since initial parameters will return with a generator.
        :return:
        """
        return self.vars

import torch
from torch.utils.data import Dataset
import numpy as np
import os
import random

class Normal_Loader(Dataset):
    """
    is_train = 1 <- train, 0 <- test
    """
    def __init__(self, is_train=1, path='/content/drive/MyDrive/UCF/'):
        super(Normal_Loader, self).__init__()
        self.is_train = is_train
        self.path = path
        if self.is_train == 1:
            data_list = os.path.join(path, 'train_normal.txt')
            with open(data_list, 'r') as f:
                self.data_list = f.readlines()
        else:
            data_list = os.path.join(path, 'test_normalv2.txt')
            with open(data_list, 'r') as f:
                self.data_list = f.readlines()
            random.shuffle(self.data_list)
            self.data_list = self.data_list[:-10]
    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        if self.is_train == 1:
            rgb_npy = np.load(os.path.join(self.path+'all_rgbs', self.data_list[idx][:-1]+'.npy'))
            flow_npy = np.load(os.path.join(self.path+'all_flows', self.data_list[idx][:-1]+'.npy'))
            concat_npy = np.concatenate([rgb_npy, flow_npy], axis=1)
            return concat_npy
        else:
            name, frames, gts = self.data_list[idx].split(' ')[0], int(self.data_list[idx].split(' ')[1]), int(self.data_list[idx].split(' ')[2][:-1])
            rgb_npy = np.load(os.path.join(self.path+'all_rgbs', name + '.npy'))
            flow_npy = np.load(os.path.join(self.path+'all_flows', name + '.npy'))
            concat_npy = np.concatenate([rgb_npy, flow_npy], axis=1)
            return concat_npy, gts, frames , name

class Anomaly_Loader(Dataset):
    """
    is_train = 1 <- train, 0 <- test
    """
    def __init__(self, is_train=1, path='/content/drive/MyDrive/UCF/'):
        super(Anomaly_Loader, self).__init__()
        self.is_train = is_train
        self.path = path
        if self.is_train == 1:
            data_list = os.path.join(path, 'train_anomaly.txt')
            with open(data_list, 'r') as f:
                self.data_list = f.readlines()
        else:
            data_list = os.path.join(path, 'test_anomalyv2.txt')
            with open(data_list, 'r') as f:
                self.data_list = f.readlines()

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        if self.is_train == 1:
            rgb_npy = np.load(os.path.join(self.path+'all_rgbs', self.data_list[idx][:-1]+'.npy'))
            flow_npy = np.load(os.path.join(self.path+'all_flows', self.data_list[idx][:-1]+'.npy'))
            concat_npy = np.concatenate([rgb_npy, flow_npy], axis=1)
            return concat_npy
        else:
            name, frames, gts = self.data_list[idx].split('|')[0], int(self.data_list[idx].split('|')[1]), self.data_list[idx].split('|')[2][1:-2].split(',')
            gts = [int(i) for i in gts]
            print(gts)
            rgb_npy = np.load(os.path.join(self.path+'all_rgbs', name + '.npy'))
            flow_npy = np.load(os.path.join(self.path+'all_flows', name + '.npy'))
            concat_npy = np.concatenate([rgb_npy, flow_npy], axis=1)
            return concat_npy, gts, frames , name

if __name__ == '__main__':
    loader2 = Normal_Loader(is_train=0)
    print(len(loader2))

!mkdir checkpoint

from torch.utils.data import DataLoader
import os
from sklearn import metrics
import argparse


LR = 0.001
W = 0.0010000000474974513
MODALITY = 'TWO'
INPUT_DIM = 2048
DROP = 0.6
FFC = False

best_auc = 0

normal_train_dataset = Normal_Loader(is_train=1)
normal_test_dataset = Normal_Loader(is_train=0)

anomaly_train_dataset = Anomaly_Loader(is_train=1)
anomaly_test_dataset = Anomaly_Loader(is_train=0)

normal_train_loader = DataLoader(normal_train_dataset, batch_size=30, shuffle=True)
normal_test_loader = DataLoader(normal_test_dataset, batch_size=1, shuffle=True)

anomaly_train_loader = DataLoader(anomaly_train_dataset, batch_size=30, shuffle=True)
anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=1, shuffle=True)

device = 'cpu'

if FFC:
    model = Learner2(input_dim=INPUT_DIM, drop_p=DROP).to(device)
else:
    model = Learner(input_dim=INPUT_DIM, drop_p=DROP).to(device)

optimizer = torch.optim.Adagrad(model.parameters(), lr= LR, weight_decay=W)
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50] ,gamma=0.1)
criterion = MIL

def save_checkpoint(epoch, train_loss):
    print('Saving best checkpoint... Epoch:', epoch)
    state = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
    }

    if not os.path.isdir('/content/drive/MyDrive/UCF/checkpoint'):
        os.mkdir('/content/drive/MyDrive/UCF/checkpoint')
    best_checkpoint_path = f'/content/drive/MyDrive/UCF/checkpoint/ckpt_epoch_BEST.pth'
    torch.save(state, best_checkpoint_path)

def train(epoch):
    print('\nEpoch: %d' % epoch)
    model.train()
    train_loss = 0
    best_epoch_loss = float('inf')
    correct = 0
    total = 0
    for batch_idx, (normal_inputs, anomaly_inputs) in enumerate(zip(normal_train_loader, anomaly_train_loader)):
        inputs = torch.cat([anomaly_inputs, normal_inputs], dim=1)
        batch_size = inputs.shape[0]
        inputs = inputs.view(-1, inputs.size(-1)).to('cpu')
        outputs = model(inputs)
        loss = criterion(outputs, batch_size)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        print(train_loss)
    print('loss = {}', train_loss/len(normal_train_loader))
    if train_loss/len(normal_train_loader) < best_epoch_loss:
            best_epoch_loss = train_loss/len(normal_train_loader)
            save_checkpoint(epoch, best_epoch_loss)
    scheduler.step()


def test_abnormal(epoch):
    model.eval()
    global best_auc
    auc = 0
    with torch.no_grad():
        for i, (data, data2) in enumerate(zip(anomaly_test_loader, normal_test_loader)):
            inputs, gts, frames , name = data
            inputs = inputs.view(-1, inputs.size(-1)).to('cpu')
            score = model(inputs)
            score = score.cpu().detach().numpy()
            score_list = np.zeros(frames[0])
            step = np.round(np.linspace(0, frames[0]//16, 33))

            for j in range(32):
                score_list[int(step[j])*16:(int(step[j+1]))*16] = score[j]

            gt_list = np.zeros(frames[0])
            for k in range(len(gts)//2):
                s = gts[k*2]
                e = min(gts[k*2+1], frames)
                gt_list[s-1:e] = 1

            inputs2, gts2, frames2 , name2 = data2
            inputs2 = inputs2.view(-1, inputs2.size(-1)).to('cpu')
            score2 = model(inputs2)
            score2 = score2.cpu().detach().numpy()
            score_list2 = np.zeros(frames2[0])
            step2 = np.round(np.linspace(0, frames2[0]//16, 33))
            for kk in range(32):
                score_list2[int(step2[kk])*16:(int(step2[kk+1]))*16] = score2[kk]
            gt_list2 = np.zeros(frames2[0])
            score_list3 = np.concatenate((score_list, score_list2), axis=0)
            gt_list3 = np.concatenate((gt_list, gt_list2), axis=0)

            fpr, tpr, thresholds = metrics.roc_curve(gt_list3, score_list3, pos_label=1)
            auc += metrics.auc(fpr, tpr)

        print('auc = {}',auc/140)

        if best_auc < auc/140:
            print('Saving..')
            state = {
                'net': model.state_dict(),
            }
            if not os.path.isdir('/content/drive/MyDrive/UCF/checkpoint'):
                os.mkdir('/content/drive/MyDrive/UCF/checkpoint')
            torch.save(state, '/content/drive/MyDrive/UCF/checkpoint/ckpt.pth')
            best_auc = auc/140

for epoch in range(0, 100):
    train(epoch)
    test_abnormal(epoch)

from __future__ import division
import torch
from torch.nn import init
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
from functools import partial
import pdb

__all__ = ['ResNeXt', 'resnet50', 'resnet101']


def conv3x3x3(in_planes, out_planes, stride=1):
    # 3x3x3 convolution with padding
    return nn.Conv3d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=1,
        bias=False)


def downsample_basic_block(x, planes, stride):
    out = F.avg_pool3d(x, kernel_size=1, stride=stride)
    zero_pads = torch.Tensor(
        out.size(0), planes - out.size(1), out.size(2), out.size(3),
        out.size(4)).zero_()
    if isinstance(out.data, torch.cuda.FloatTensor):
        zero_pads = zero_pads.cuda()

    out = Variable(torch.cat([out.data, zero_pads], dim=1))

    return out


class ResNeXtBottleneck(nn.Module):
    expansion = 2

    def __init__(self, inplanes, planes, cardinality, stride=1,
                 downsample=None):
        super(ResNeXtBottleneck, self).__init__()
        mid_planes = cardinality * int(planes / 32)
        self.conv1 = nn.Conv3d(inplanes, mid_planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm3d(mid_planes)
        self.conv2 = nn.Conv3d(
            mid_planes,
            mid_planes,
            kernel_size=3,
            stride=stride,
            padding=1,
            groups=cardinality,
            bias=False)
        self.bn2 = nn.BatchNorm3d(mid_planes)
        self.conv3 = nn.Conv3d(
            mid_planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm3d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class ResNeXt(nn.Module):

    def __init__(self,
                 block,
                 layers,
                 sample_size,
                 sample_duration,
                 shortcut_type='B',
                 cardinality=32,
                 num_classes=400,
                 input_channels=3,
                 output_layers=[]):
        self.inplanes = 64
        super(ResNeXt, self).__init__()
        self.conv1 = nn.Conv3d(
            input_channels,
            64,
            kernel_size=7,
            stride=(1, 2, 2),
            padding=(3, 3, 3),
            bias=False)

        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)
        self.layer1 = self._make_layer(block, 128, layers[0], shortcut_type,
                                       cardinality)
        self.layer2 = self._make_layer(
            block, 256, layers[1], shortcut_type, cardinality, stride=2)
        self.layer3 = self._make_layer(
            block, 512, layers[2], shortcut_type, cardinality, stride=2)
        self.layer4 = self._make_layer(
            block, 1024, layers[3], shortcut_type, cardinality, stride=2)
        last_duration = int(math.ceil(sample_duration / 16))
        last_size = int(math.ceil(sample_size / 32))
        self.avgpool = nn.AdaptiveAvgPool3d(1)
        self.fc = nn.Linear(cardinality * 32 * block.expansion, num_classes)

        #layer to output on forward pass
        self.output_layers = output_layers

        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')
            elif isinstance(m, nn.BatchNorm3d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self,
                    block,
                    planes,
                    blocks,
                    shortcut_type,
                    cardinality,
                    stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            if shortcut_type == 'A':
                downsample = partial(
                    downsample_basic_block,
                    planes=planes * block.expansion,
                    stride=stride)
            else:
                downsample = nn.Sequential(
                    nn.Conv3d(
                        self.inplanes,
                        planes * block.expansion,
                        kernel_size=1,
                        stride=stride,
                        bias=False), nn.BatchNorm3d(planes * block.expansion))

        layers = []
        layers.append(
            block(self.inplanes, planes, cardinality, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes, cardinality))

        return nn.Sequential(*layers)

    def forward(self, x):
        #pdb.set_trace()
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x1 = self.layer1(x)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)

        x5 = self.avgpool(x4)

        x6 = x5.view(x5.size(0), -1)
        x7 = self.fc(x6)

        if len(self.output_layers) == 0:
            return x7, x6
        else:
            out = []
            out.append(x7)
            for i in self.output_layers:
                if i == 'avgpool':
                    out.append(x6)
                if i == 'layer4':
                    out.append(x4)
                if i == 'layer3':
                    out.append(x3)

        return out

    def freeze_batch_norm(self):
        for name,m in self.named_modules():
            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm3d): # PHIL: i Think we can write just  "if isinstance(m, nn._BatchNorm)
                m.eval() # use mean/variance from the training
                m.weight.requires_grad = False
                m.bias.requires_grad = False


def get_fine_tuning_parameters(model, ft_begin_index):
    if ft_begin_index == 0:
        return model.parameters()

    ft_module_names = []
    for i in range(ft_begin_index, 5):
        ft_module_names.append('layer{}'.format(i))
    ft_module_names.append('fc')

    print("Layers to finetune : ", ft_module_names)

    parameters = []
    for k, v in model.named_parameters():
        for ft_module in ft_module_names:
            if ft_module in k:
                parameters.append({'params': v})
                break
        else:
            parameters.append({'params': v, 'lr': 0.0})

    return parameters


def resnet50(**kwargs):
    """Constructs a ResNet-50 model.
    """
    model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], **kwargs)
    return model


def resnet101(**kwargs):
    """Constructs a ResNet-101 model.
    """
    model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], **kwargs)
    return model


def resnet152(**kwargs):
    """Constructs a ResNet-101 model.
    """
    model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], **kwargs)
    return model

from __future__ import division
import torch
from torch import nn

def generate_model():
    model = resnet101(
        num_classes=400,
        shortcut_type='B',
        cardinality=32,
        sample_size=112,
        sample_duration=16,
        input_channels=3,
        output_layers=[]
    )

    model = nn.DataParallel(model)

    return model

import torch
import glob
import numpy as np
import os
import subprocess

import torchvision.transforms as transforms
import torch.nn.functional as F
from torch import nn
from PIL import Image, ImageFilter

from PIL import Image, ImageFilter, ImageOps, ImageChops
import numpy as np
import torch
import random
import numbers
import pdb
import time
import cv2
from matplotlib import pyplot as plt
from tqdm import tqdm
import sys
import argparse

class ToTensor(object):
    def __init__(self, norm_value=255):
        self.norm_value = norm_value

    def __call__(self, pic):
        if isinstance(pic, np.ndarray):
            img = torch.from_numpy(pic.transpose((2, 0, 1)))
            return img.float().div(self.norm_value)

        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
        nchannel = len(pic.mode)
        img = img.view(pic.size[1], pic.size[0], nchannel)
        img = img.transpose(0, 1).transpose(0, 2).contiguous()
        return img.float().div(self.norm_value)

    def randomize_parameters(self):
        pass

class Normalize(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        for t, m, s in zip(tensor, self.mean, self.std):
            t.sub_(m).div_(s)
        return tensor

    def randomize_parameters(self):
        pass

import cv2
import os

def video_to_frames(video_path, output_folder):
    # Get the video name without extension
    video_name = os.path.splitext(os.path.basename(video_path))[0]

    # Create a folder for the current video
    video_output_folder = os.path.join(output_folder, video_name)

    # Skip conversion if the output folder already exists and contains frames
    if os.path.exists(video_output_folder) and os.listdir(video_output_folder):
        print(f"Skipping {video_name} as it has already been converted.")
        return

    # Create the output folder if it does not exist
    if not os.path.exists(video_output_folder):
        os.makedirs(video_output_folder)

    # Open the video file
    cap = cv2.VideoCapture(video_path)

    # Check if the video file opened successfully
    if not cap.isOpened():
        print(f"Error opening video file: {video_path}")
        return

    frame_count = 0

    while True:
        # Read a frame from the video
        ret, frame = cap.read()

        # If no frame is returned, then the video has ended
        if not ret:
            break

        # Construct the output file path
        output_file = os.path.join(video_output_folder, f"frame_{frame_count:04d}.jpg")

        # Save the frame as an image file
        cv2.imwrite(output_file, frame)

        frame_count += 1

    # Release the video capture object
    cap.release()

    print(f"Video {video_name} has been successfully converted to frames and saved in {video_output_folder}")

# Example usage
video_path = '/content/drive/MyDrive/UCF-Crime/Training-Normal-Videos-Part-2/Normal_Videos483_x264.mp4'
output_folder = '/content/drive/MyDrive/UCF-Crime/Normal'
video_to_frames(video_path, output_folder)

import torch
import glob
import numpy as np
import os
import cv2
from PIL import Image
from torchvision import transforms
import torch.nn.functional as F
from tqdm import tqdm
import time
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText


# Define your model and classifier (assuming these are predefined elsewhere)
model = generate_model()  # feature extractor
classifier = Learner()  # classifier

checkpoint = torch.load('/content/drive/MyDrive/UCF/checkpoint/ckpt_epoch_BEST.pth')
classifier.load_state_dict(checkpoint['model_state_dict'])

model.eval()
classifier.eval()

def send_alert(subject, body):
  print(subject)
  print(body)

def load_location_data(json_path):
    with open(json_path, 'r') as f:
        return json.load(f)

location_data_path = '/content/location.json'
location_data = load_location_data(location_data_path)



# Define transformation
transform = transforms.Compose([
    transforms.Resize((240, 320)),
    transforms.ToTensor(),
])

# Directory containing the images
image_dir = '/content/drive/MyDrive/UCF-Crime/Anomaly/Explosion027_x264'
save_path = image_dir + '_result'
os.makedirs(save_path, exist_ok=True)

# Get all image files and sort them
img_files = sorted(glob.glob(os.path.join(image_dir, '*')))

# Prepare for video writing
output_video_path = '/content/drive/MyDrive/UCF-Crime/Exp_video.mp4'
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_video_path, fourcc, 30, (320, 240))

abnormal_count = 0
abnormal_threshold = 0.3

video_location = location_data.get(os.path.basename(image_dir), {})
latitude = video_location.get("City")
longitude = video_location.get("Street")


# Initialize the input tensor
inputs = torch.zeros((1, 3, 16, 240, 320))  # (batch_size, channels, num_frames, height, width)

# Process images
for num, img_file in enumerate(tqdm(img_files)):
    img = transform(Image.open(img_file))

    if num < 16:
        inputs[0, :, num, :, :] = img  # Fill the frames initially
    else:
        inputs[0, :, :15, :, :] = inputs[0, :, 1:, :, :]  # Shift frames to the left
        inputs[0, :, 15, :, :] = img  # Add the new frame

        with torch.no_grad():
            start = time.time()
            output, feature = model(inputs)
            feature = F.normalize(feature, p=2, dim=1)
            out = classifier(feature)
            end = time.time()

        FPS = str(1 / (end - start))[:5]
        out_str = str(out.item())[:5]

        cv_img = cv2.imread(img_file)
        h, w, _ = cv_img.shape
        cv_img = cv2.putText(cv_img, f'FPS: {FPS} Pred: {out_str}', (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 240), 2)

        if out.item() > abnormal_threshold:
            abnormal_count += 1
            cv_img = cv2.rectangle(cv_img, (0, 0), (w, h), (0, 0, 255), 3)
            alert_subject = "Abnormal Activity Detected"
            alert_body = f"Abnormal activity detected at frame {num}. Location: City: {latitude}, Street: {longitude}"
            send_alert(alert_subject, alert_body)

        save_img_path = os.path.join(save_path, os.path.basename(img_file))
        cv2.imwrite(save_img_path, cv_img)
        video_writer.write(cv_img)

video_writer.release()

# Calculate the percentage of abnormal frames
abnormal_percentage = (abnormal_count / len(img_files)) * 100

# Decide overall classification based on the abnormal percentage
overall_classification = 'Abnormal' if abnormal_percentage >= 50 else 'Normal'

print(f"Video saved at {output_video_path}")
print(f"Overall Classification: {overall_classification}")
print(f"Abnormal Percentage: {abnormal_percentage}%")

import torch
import glob
import numpy as np
import os
import cv2
from PIL import Image
from torchvision import transforms
import torch.nn.functional as F
from tqdm import tqdm
import time
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import json

# Define your model and classifier (assuming these are predefined elsewhere)
model = generate_model()  # feature extractor
classifier = Learner()  # classifier

checkpoint = torch.load('/content/drive/MyDrive/UCF/checkpoint/ckpt_epoch_BEST.pth')
classifier.load_state_dict(checkpoint['model_state_dict'])

model.eval()
classifier.eval()

def send_alert(subject, body):
  print(subject)
  print(body)

def load_location_data(json_path):
    with open(json_path, 'r') as f:
        return json.load(f)

location_data_path = '/content/location.json'
location_data = load_location_data(location_data_path)



# Define transformation
transform = transforms.Compose([
    transforms.Resize((240, 320)),
    transforms.ToTensor(),
])

# Directory containing the images
image_dir = '/content/drive/MyDrive/UCF-Crime/Normal/Normal_Videos_014_x264'
save_path = image_dir + '_result'
os.makedirs(save_path, exist_ok=True)

# Get all image files and sort them
img_files = sorted(glob.glob(os.path.join(image_dir, '*')))

# Prepare for video writing
output_video_path = '/content/drive/MyDrive/UCF-Crime/noo_video.mp4'
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter(output_video_path, fourcc, 30, (320, 240))

abnormal_count = 0
abnormal_threshold = 0.5

video_location = location_data.get(os.path.basename(image_dir), {})
latitude = video_location.get("City")
longitude = video_location.get("Street")


# Initialize the input tensor
inputs = torch.zeros((1, 3, 16, 240, 320))  # (batch_size, channels, num_frames, height, width)

# Process images
for num, img_file in enumerate(tqdm(img_files)):
    img = transform(Image.open(img_file))

    if num < 16:
        inputs[0, :, num, :, :] = img  # Fill the frames initially
    else:
        inputs[0, :, :15, :, :] = inputs[0, :, 1:, :, :]  # Shift frames to the left
        inputs[0, :, 15, :, :] = img  # Add the new frame

        with torch.no_grad():
            start = time.time()
            output, feature = model(inputs)
            feature = F.normalize(feature, p=2, dim=1)
            out = classifier(feature)
            end = time.time()

        FPS = str(1 / (end - start))[:5]
        out_str = str(out.item())[:5]

        cv_img = cv2.imread(img_file)
        h, w, _ = cv_img.shape
        cv_img = cv2.putText(cv_img, f'FPS: {FPS} Pred: {out_str}', (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 240), 2)

        if out.item() > abnormal_threshold:
            abnormal_count += 1
            cv_img = cv2.rectangle(cv_img, (0, 0), (w, h), (0, 0, 255), 3)
            alert_subject = "Abnormal Activity Detected"
            alert_body = f"Abnormal activity detected at frame {num}. Location: City: {latitude}, Street: {longitude}"
            send_alert(alert_subject, alert_body)

        save_img_path = os.path.join(save_path, os.path.basename(img_file))
        cv2.imwrite(save_img_path, cv_img)
        video_writer.write(cv_img)

video_writer.release()

# Calculate the percentage of abnormal frames
abnormal_percentage = (abnormal_count / len(img_files)) * 100

# Decide overall classification based on the abnormal percentage
overall_classification = 'Abnormal' if abnormal_percentage >= 50 else 'Normal'

print(f"Video saved at {output_video_path}")
print(f"Overall Classification: {overall_classification}")
print(f"Abnormal Percentage: {abnormal_percentage}%")